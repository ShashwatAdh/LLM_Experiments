# LLM_Experiments
Trying and Learning LLM Models

 ## Experiments
This model is a fine-tuned version of __alexsherstinsky/Mistral-7B-v0.1-sharded__ 

  Mistral-7B-v0.1 is a transformer model, with the following architecture choices:

    Grouped-Query Attention
    Sliding-Window Attention
    Byte-fallback BPE tokenizer

HoangCuongNguyen/CTI-to-MITRE-dataset is used to fine-tune the LLM  model
